{"name":"Jacob Panikulam","tagline":"whoisJake","body":"Jacob Panikulam\r\n================\r\n\r\n\r\n# My Current Projects\r\n\r\n\r\n## [Amazon Picking Challenge](http://amazonpickingchallenge.org/details.shtml)\r\n\r\nI competed with Team Georgia Tech in the Amazon Picking challenge using a Schunk robot with two 7-DOF arms. We came in 10th place, internationally (40+ teams qualified, 28 attended). I designed and integrated the majority of the perception stack, including PCL in C++, Image analysis in MATLAB and various vision and logic components in Python. All of this was done using ROS and the various tools it provides, alongside numerous internally developed tools.\r\n\r\nI competed as one of four members of Team Georgia Tech out of GT's Robotics and Intelligent Machines laboratory.\r\n\r\n\r\n#### Implementation details\r\nImage-similarity metrics are generated using symmetrized Kullbeck-Leibler divergence (Jensen-Shannon) for initial object recognition in the 2D image. The object is segmented in 2D, and that segmentation is projected onto the point cloud - and is then used as a seed for region-growing 3D segmentation. Then, prerejective sample consensus is used to register our known models of the object to the sensed point cloud, which enables our robot to autonomously grasp the object.\r\n\r\n\r\n## [Propagator](http://mil.ufl.edu/propagator/)\r\nPropagator is an autonomous boat that competes in the AUVSI robo-boat competition. Our team will be competing in Maritime RobotX in Hawaii in 2016.\r\n\r\n\r\nI implemented a constrained nonlinear control allocation method for our boat's azimuth thrusters by adapting [Tor Johansen's method](http://www.fossen.biz/home/papers/tcst04.pdf) to our needs.\r\n\r\n\r\nI am currently developing a path-planner for the boat that takes into account an approximation of water-resistance as a function of heading and minimizing the total rotation of our azimuth thrusters to minimize wear.\r\n\r\n\r\nOur code is fully [open source](www.github.com/uf-mil).\r\n\r\n\r\n## [Oqulus](http://www.oqulus.net/)\r\nI am the principle computer vision engineer at Sentinel Diagnostic Imaging, with our flagship product being Oqulus. Oqulus is the seminal product from Sentinel Diagnostic Imaging, whose goal is to autonomously provide diagnostic data targeting diseases such as diabetic retinopathy, retinitis pigmentosa, and glaucoma, to find hints of these before they have done irrevocable damage.\r\n\r\n\r\nI developed a 2D wavelet and neural-network based approach to autonomously segmenting images of the retinal funduss.\r\n\r\n\r\nI am currently working on a method for extracting the vessel topology, and autonomously distinguishing arteries from veins. The exact methods used are proprietary to the Oqulus technology, and cannot be discussed in great depth on a webpage.\r\n\r\n\r\n# Consulting Projects\r\nI started an initiative within the University of Florida IEEE student section to provide electronics and software consulting services to local companies and student organizations. We lend skilled students to these groups, and offer our expertise at no cost.\r\n\r\n\r\n## Quarter-Scale Tractor Team\r\nWe are working with the UF Quarter-Scale Tractor Team to develop tools for computer-aided navigation for obstacle course traversal.\r\n\r\n\r\n## UF Rocket Team\r\nWe are in talks with the UF rocket team to design and develop software for a robot arm. This arm will be used for their competition in May, as they are required to have their rocket be partially autonomously assembled before launch.\r\n\r\n\r\n# My Past Projects\r\n## VisAR\r\nVisAR (Visual Intelligence System via Augmented Reality, I didn't pick the name) uses an Oculus rift to display augmented reality to a user. \r\n\r\n\r\nWe use an FPGA to take in frames from two cameras mounted in front of the Rift, apply a barrel distortion to undo the Oculus Rift's lenses' pincushion distortion, and display that information to the user. At the same time, UI frames are being sent to the FPGA by a computer, and these are being overlayed with transparency onto the user's view.\r\n\r\n\r\nI developed a Vispy/OpenGL based rendering engine called \"libVisar\" that is designed to make simple Oculus Rift applications very easy to make in Python. The idea is that roboticists or other engineers without an OpenGL/DirectX background, and without access to Unity can create Oculus Rift GUI's and data displays for overlay on camera images.  \r\n\r\n\r\nSoon to be open source.\r\n\r\n\r\n## IEEE2015 Autonomous Robot \"Maverick\" [Video of Maverick's Arm](http://youtu.be/6lzLW4y0CQY)\r\nI led a 40 member team of undergraduate student engineers, building an autonomous robot for competition in the IEEE Southeastcon Competition. The goal is to build a fully autonomous robot that can navigate a randomized course to draw the word \"IEEE\" on an etch-a-sketch, rotate a rubix cube, successfully play [\"Simon Says\"](http://dalpix.com/images/Simon-Game_l.jpg) and then pick up a card. We found this to be too easy, and decided to implement Chess as well.\r\n\r\n\r\nWe used monocular camera SLAM for navigation, a 4-DOF manipulator arm for acting on the environment (Using a second camera for visual servoing), mecanum wheels for locomotion, an Intel NUC running Ubuntu and ROS Indigo for high-level control, and an XMega128A1U for low-level control.\r\n\r\n\r\nAll of our code is completely [open source](github.com/ufieeehw/IEEE2015). If you are looking to this for inspiration and can't seem to make something work, please feel free to contact me.\r\n\r\n\r\nFor 2016, I have passed on leadership to Ian Van Stralen, with mechanical lead Stephan Strassle.\r\n\r\n\r\n## [IEEE2014 Autonomous Robot \"Mr. Roboto\"](http://youtu.be/b_Ijie9VGrQ)\r\nWe built a robot for the SoutheastCon competition in March of 2014. The robot had to autonomously drive to each of three blue blocks, wait three seconds, and then fire a dart into a target. We did not know the position of the blue blocks in advance. Most of the other teams relied on following the white lines (inc. Georgia Tech, Duke, etc). We, instead, scavenged a LIDAR from a Roomba (Well, a [Neato XV-11](https://www.youtube.com/watch?v=KnspWPlBM_o)) and navigated using a particle filter. \r\n\r\n\r\nI implemented a purely computer-vision method for identifying course objectives and determining the position of a target in the camera's field of view. At the beginning of the video, the robot rotates the turret head to visually identify the blue blocks it must drive to. Then, at each block, it servos to point toward the target based on its estimate of the position. Vision is then used to correct the target position estimate, and fire the dart accurately.\r\n\r\n\r\n## [UF EFTP Bumper Bots](http://imgur.com/a/TH279)\r\nBrandon Grant and I built and programmed a set of remote-controlled bumper cars using RobotC and Lego NXT. \"Bumper Bots\" were designed for a UF Shands competition, to design a toy for ailing children at Shands hospital. We ended in the top three teams, and the kids adored our robots.\r\n\r\n\r\n# Coursework\r\n\r\n\r\n## Medical Image Analysis\r\nI am currently taking a graduate CS course at UF in medical image analysis. The course teaches methods behind CT and MRI reconstruction, and methods in medical image segmentation/registration.\r\n\r\n\r\n## Underactuated Robotics\r\nI am currently working through MIT OCW's Underactuated Robotics course. I've gained a lot from this course already, and have been using tools gained in my daily work on robots.\r\n\r\n\r\n## Convex Optimization\r\nI am also currently working through (Albeit at a much lower priority) Dr. Stephen Boyd's Convex Optimization lectures from Stanford, at the encouragement of my friend Eric.\r\n\r\n\r\n# Interests\r\n\r\n\r\nAbove all, I like jokes and movies about space. On the technical side, I am interested in autonomous systems, with preference to vehicles. Or really anything that you can convince me is a cool enough math/software problem. Key words: Vision, controls, autonomous, Linux, python, linear algebra, microcontrollers.","google":"UA-63284725-1","note":"Don't delete this file! It's used internally to help with page regeneration."}