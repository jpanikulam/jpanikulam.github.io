{"name":"Jacob Panikulam","tagline":"whoisJake","body":"Jacob Panikulam\r\n================\r\n\r\n# My Current Projects\r\n\r\n## [Subjugator](http://subjugator.org/)\r\nI was named Subjugator software lead at the end of August 2015. Since then, we've begun a massive campaign to re-implement all of the existing code, which has a few target tasks:\r\n\r\nCurrent features include:\r\n\r\n* An alarm system for sharing alarm states across the various drivers and planners running on the robot.\r\n* An internally implemented underwater dynamics simulator including\r\n    * Simulated [imaging sonar](http://sites.garmin.com/en-US/panoptix/) - We could really use one of these in real life!\r\n    * Simulated [Doppler Velocity Log](http://www.rdinstruments.com/navigator.aspx)\r\n    * Simulated cameras (Passing data over ROS messages, directly from their own framebuffers, sonar works the same way)\r\n    * Underwater dynamics, including simple water resistance\r\n        * Soon-to-be-developed: GPU-estimated surface area estimation for more accurate water resistance\r\n    * Many, many visualization widgets for viewing the behavior of the sub\r\n    * Unit-tests *in* simulation - running physics-only simulation to prove the sub's functionality\r\n\r\n* A monte-carlo tool for randomized simulated testing\r\n    * Simulate many thousands of randomized start positions to test a controller\r\n        * Verify the stability of the controller, i.e. after `T` seconds, the sub remains within an n-ball of radius `r`, with less than `epsilon` distance from the targeted point\r\n    * Deterine optimal gains in simulation via Monte-Carlo (LQR is not sufficient due to thrust constraints and nonlinear dynamics)\r\n    * And optimize any other item that one might choose\r\n\r\n* Continuous integration using SemaphoreCI\r\n* Thorough unit-tests (Though, we're not quite at 100% coverage)\r\n* Thruster mapping using slsqp, which we use as a bounded least-squares solver\r\n* ROS-drivers for VideoRay M5 thrusters\r\n\r\nSome current projects include:\r\n* 3D motion planning\r\n* 3D traversability map generated from sensed information, not hardcoded\r\n* [Highly parallel motion planning](http://dspace.mit.edu/openaccess-disseminate/1721.1/81448) using CUDA\r\n* Model-predictive controller using CVXGEN\r\n\r\nWe have many more, but won't discuss them until they are in progress.\r\n\r\n## [Propagator](http://mil.ufl.edu/propagator/)\r\nPropagator is an autonomous boat that competes in the AUVSI robo-boat competition. Our team will be competing in the Maritime RobotX competition in Hawaii in 2016.\r\n\r\n\r\nI implemented a constrained nonlinear control allocation method for our boat's azimuth thrusters by adapting [Tor Johansen's method](http://www.fossen.biz/home/papers/tcst04.pdf) to our needs.\r\n\r\nThere are a few current projects I am working on or managing, in parallel:\r\n\r\n* Motion planning using kinodynamic RRT* (So far, this works spectacularly)\r\n* Mapping and obstacle detection using a Velodyne puck (We finally got out Velodyne about two weeks ago)\r\n* Object tracking using Velodyne-generated pcl data\r\n\r\nWith more on the queue once these are complete.\r\n\r\nOur code is fully [open source](www.github.com/uf-mil).\r\n\r\n\r\n## [CVXBind](https://github.com/jpanikulam/cvxbind)\r\nAutomatically generate bindings Boost::Python Python bindings for C-code generated by CVXGEN.\r\n\r\n\r\n## [Oqulus](http://www.oqulus.net/)\r\nI am the principle computer vision engineer at Sentinel Diagnostic Imaging, with our flagship product being Oqulus. Oqulus is the seminal product from Sentinel Diagnostic Imaging, whose goal is to autonomously provide diagnostic data targeting diseases such as diabetic retinopathy, retinitis pigmentosa, and glaucoma, to find hints of these before they have done irrevocable damage.\r\n\r\n\r\nI developed a 2D wavelet and neural-network based approach to autonomously segmenting images of the retinal fundus.\r\n\r\n\r\nI am currently working on a method for extracting the vessel topology, and autonomously distinguishing arteries from veins. The exact methods used are proprietary to the Oqulus technology, and cannot be discussed in great depth on a webpage.\r\n\r\n\r\n# Consulting Projects\r\nI started an initiative within the University of Florida IEEE student section to provide electronics and software consulting services to local companies and student organizations. We lend skilled students to these groups, and offer our expertise at no cost.\r\n\r\n\r\n## Quarter-Scale Tractor Team\r\nWe are working with the UF Quarter-Scale Tractor Team to develop tools for computer-aided navigation for obstacle course traversal.\r\n\r\n\r\n## UF Rocket Team\r\nWe are in talks with the UF rocket team to design and develop software for a robot arm. This arm will be used for their competition in May, as they are required to have their rocket be partially autonomously assembled before launch.\r\n\r\n\r\n# My Past Projects\r\n## [Amazon Picking Challenge](http://amazonpickingchallenge.org/details.shtml)\r\n\r\nI competed with Team Georgia Tech in the Amazon Picking challenge using a Schunk robot with two 7-DOF arms. We came in 10th place, internationally (40+ teams qualified, 28 attended). I designed and integrated the majority of the perception stack, including PCL in C++, Image analysis in MATLAB and various vision and logic components in Python. All of this was done using ROS and the various tools it provides, alongside numerous internally developed tools.\r\n\r\nI competed as one of four members of Team Georgia Tech out of GT's Robotics and Intelligent Machines laboratory.\r\n\r\nOur code is [open source](https://github.com/ehuang3/apc_ros)\r\n\r\n#### Implementation details\r\nImage-similarity metrics are generated using symmetrized Kullbeck-Leibler divergence (Jensen-Shannon) for initial object recognition in the 2D image. The object is segmented in 2D, and that segmentation is projected onto the point cloud - and is then used as a seed for region-growing 3D segmentation. Then, prerejective sample consensus is used to register our known models of the object to the sensed point cloud, which enables our robot to autonomously grasp the object.\r\n\r\n\r\n## VisAR\r\nVisAR (Visual Intelligence System via Augmented Reality, I didn't pick the name) uses an Oculus rift to display augmented reality to a user.\r\n\r\n\r\nWe use an FPGA to take in frames from two cameras mounted in front of the Rift, apply a barrel distortion to undo the Oculus Rift's lenses' pincushion distortion, and display that information to the user. At the same time, UI frames are being sent to the FPGA by a computer, and these are being overlayed with transparency onto the user's view.\r\n\r\n\r\nI developed a Vispy/OpenGL based rendering engine called \"libVisar\" that is designed to make simple Oculus Rift applications very easy to make in Python. The idea is that roboticists or other engineers without an OpenGL/DirectX background, and without access to Unity can create Oculus Rift GUI's and data displays for overlay on camera images.\r\n\r\n\r\nSoon to be open source.\r\n\r\n\r\n## IEEE2015 Autonomous Robot \"Maverick\" [Video of Maverick's Arm](http://youtu.be/6lzLW4y0CQY)\r\nI led a 40 member team of undergraduate student engineers, building an autonomous robot for competition in the IEEE Southeastcon Competition. The goal is to build a fully autonomous robot that can navigate a randomized course to draw the word \"IEEE\" on an etch-a-sketch, rotate a rubix cube, successfully play [\"Simon Says\"](http://dalpix.com/images/Simon-Game_l.jpg) and then pick up a card. We found this to be too easy, and decided to implement Chess as well.\r\n\r\n\r\nWe used monocular camera SLAM for navigation, a 4-DOF manipulator arm for acting on the environment (Using a second camera for visual servoing), mecanum wheels for locomotion, an Intel NUC running Ubuntu and ROS Indigo for high-level control, and an XMega128A1U for low-level control.\r\n\r\n\r\nAll of our code is completely [open source](github.com/ufieeehw/IEEE2015). If you are looking to this for inspiration and can't seem to make something work, please feel free to contact me.\r\n\r\n\r\nFor 2016, I have passed on leadership to Ian Van Stralen, with mechanical lead Stephan Strassle.\r\n\r\n\r\n## [IEEE2014 Autonomous Robot \"Mr. Roboto\"](http://youtu.be/b_Ijie9VGrQ)\r\nWe built a robot for the SoutheastCon competition in March of 2014. The robot had to autonomously drive to each of three blue blocks, wait three seconds, and then fire a dart into a target. We did not know the position of the blue blocks in advance. Most of the other teams relied on following the white lines (inc. Georgia Tech, Duke, etc). We, instead, scavenged a LIDAR from a Roomba (Well, a [Neato XV-11](https://www.youtube.com/watch?v=KnspWPlBM_o)) and navigated using a particle filter.\r\n\r\n\r\nI implemented a purely computer-vision method for identifying course objectives and determining the position of a target in the camera's field of view. At the beginning of the video, the robot rotates the turret head to visually identify the blue blocks it must drive to. Then, at each block, it servos to point toward the target based on its estimate of the position. Vision is then used to correct the target position estimate, and fire the dart accurately.\r\n\r\n\r\n## [UF EFTP Bumper Bots](http://imgur.com/a/TH279)\r\nBrandon Grant and I built and programmed a set of remote-controlled bumper cars using RobotC and Lego NXT. \"Bumper Bots\" were designed for a UF Shands competition, to design a toy for ailing children at Shands hospital. We ended in the top three teams, and the kids adored our robots.\r\n\r\n\r\n# Coursework\r\n\r\n## Nonlinear Controls\r\nI am currently taking Dr. Warren Dixon's graduate nonlinear controls course. So far, it has been largely a review of Tedrake's underactuated robotics.\r\n\r\n## Medical Image Analysis\r\nGraduate CS course at UF in medical image analysis. The course teaches methods behind CT and MRI reconstruction, and methods in medical image segmentation/registration.\r\n\r\n\r\n## Underactuated Robotics\r\nMIT OCW's graduate Underactuated Robotics course. I've gained a lot from this course, and have been using tools gained in my daily work.\r\n\r\n\r\n## Convex Optimization\r\nI am also currently working through (Albeit at a much lower priority) Dr. Stephen Boyd's Convex Optimization lectures from Stanford, at the encouragement of my friend Eric.\r\n\r\n\r\nIf you're coming here from MIL, the other courses I suggest are:\r\n\r\n[Khan Academy Linear Algebra](https://www.khanacademy.org/math/linear-algebra), Skip everything you already know, it's all a lead-up to the important stuff, which happens in the third lecture set.\r\n\r\n[MIT OCW Intro to Algorithms](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/lecture-videos/)\r\n\r\n[Udacity Intro to AI for robotics](https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373)\r\n\r\nI gained a huge amount from these. All are free. Don't let anybody make you pay for knowledge on the internet.\r\n\r\n\r\n# Interests\r\n\r\n\r\nAbove all, I like jokes and movies about space. On the technical side, I am interested in autonomous systems, with preference to vehicles. Or really anything that you can convince me is a cool enough math/software problem. Key words: Perception, controls, autonomous, Linux, python, linear algebra, motion planning, model-predictive control, simulation, microcontrollers.","google":"UA-63284725-1","note":"Don't delete this file! It's used internally to help with page regeneration."}