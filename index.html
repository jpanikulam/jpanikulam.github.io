<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Jacob Panikulam by jpanikulam</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Jacob Panikulam</h1>
      <h2 class="project-tagline">whoisJake</h2>
    </section>

    <section class="main-content">
      <h1>
<a id="jacob-panikulam" class="anchor" href="#jacob-panikulam" aria-hidden="true"><span class="octicon octicon-link"></span></a>Jacob Panikulam</h1>

<h1>
<a id="my-current-projects" class="anchor" href="#my-current-projects" aria-hidden="true"><span class="octicon octicon-link"></span></a>My Current Projects</h1>

<h2>
<a id="subjugator" class="anchor" href="#subjugator" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://subjugator.org/">Subjugator</a>
</h2>

<p>I was named Subjugator software lead at the end of August 2015. Since then, we've begun a massive campaign to re-implement all of the existing code, which has a few target tasks:</p>

<ul>
<li>Remove, replace, or fix all of the existing legacy code (Kill the cruft!)</li>
<li>Begin a shift to C++ (Dr. Schwartz has promised an imaging sonar if we do this, but after miles of negotiation, we will never be entirely replacing Python)</li>
<li>Start over, with a clear architecture in mind, design requirements in advance, and a properly build-mastered repository</li>
<li>An alarm-based fault-handling system</li>
</ul>

<p>Once that's done, we'll be working on 3D perception via stereo vision, until we can get an imaging sonar. 
Our less lofty goals include:</p>

<ul>
<li>3D motion planning</li>
<li>3D traversability map generated from sensed information, not hardcoded</li>
<li>
<a href="http://dspace.mit.edu/openaccess-disseminate/1721.1/81448">Highly parallel motion planning</a> using CUDA</li>
</ul>

<p>We have many more, but won't discuss them until they are in progress.</p>

<h2>
<a id="propagator" class="anchor" href="#propagator" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://mil.ufl.edu/propagator/">Propagator</a>
</h2>

<p>Propagator is an autonomous boat that competes in the AUVSI robo-boat competition. Our team will be competing in the Maritime RobotX competition in Hawaii in 2016.</p>

<p>I implemented a constrained nonlinear control allocation method for our boat's azimuth thrusters by adapting <a href="http://www.fossen.biz/home/papers/tcst04.pdf">Tor Johansen's method</a> to our needs.</p>

<p>There are a few current projects I am working on or managing, in parallel:</p>

<ul>
<li>Motion planning using kinodynamic RRT* (So far, this works spectacularly)</li>
<li>Mapping and obstacle detection using a Velodyne puck (We finally got out Velodyne about two weeks ago)</li>
<li>Object tracking using Velodyne-generated pcl data</li>
</ul>

<p>With more on the queue once these are complete.</p>

<p>Our code is fully <a href="www.github.com/uf-mil">open source</a>.</p>

<h2>
<a id="cvxbind" class="anchor" href="#cvxbind" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jpanikulam/cvxbind">CVXBind</a>
</h2>

<p>Automatically generator bindings Boost::Python Python bindings for C-code generated by CVXGEN.</p>

<h2>
<a id="oqulus" class="anchor" href="#oqulus" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://www.oqulus.net/">Oqulus</a>
</h2>

<p>I am the principle computer vision engineer at Sentinel Diagnostic Imaging, with our flagship product being Oqulus. Oqulus is the seminal product from Sentinel Diagnostic Imaging, whose goal is to autonomously provide diagnostic data targeting diseases such as diabetic retinopathy, retinitis pigmentosa, and glaucoma, to find hints of these before they have done irrevocable damage.</p>

<p>I developed a 2D wavelet and neural-network based approach to autonomously segmenting images of the retinal fundus.</p>

<p>I am currently working on a method for extracting the vessel topology, and autonomously distinguishing arteries from veins. The exact methods used are proprietary to the Oqulus technology, and cannot be discussed in great depth on a webpage.</p>

<h1>
<a id="consulting-projects" class="anchor" href="#consulting-projects" aria-hidden="true"><span class="octicon octicon-link"></span></a>Consulting Projects</h1>

<p>I started an initiative within the University of Florida IEEE student section to provide electronics and software consulting services to local companies and student organizations. We lend skilled students to these groups, and offer our expertise at no cost.</p>

<h2>
<a id="quarter-scale-tractor-team" class="anchor" href="#quarter-scale-tractor-team" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quarter-Scale Tractor Team</h2>

<p>We are working with the UF Quarter-Scale Tractor Team to develop tools for computer-aided navigation for obstacle course traversal.</p>

<h2>
<a id="uf-rocket-team" class="anchor" href="#uf-rocket-team" aria-hidden="true"><span class="octicon octicon-link"></span></a>UF Rocket Team</h2>

<p>We are in talks with the UF rocket team to design and develop software for a robot arm. This arm will be used for their competition in May, as they are required to have their rocket be partially autonomously assembled before launch.</p>

<h1>
<a id="my-past-projects" class="anchor" href="#my-past-projects" aria-hidden="true"><span class="octicon octicon-link"></span></a>My Past Projects</h1>

<h2>
<a id="amazon-picking-challenge" class="anchor" href="#amazon-picking-challenge" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://amazonpickingchallenge.org/details.shtml">Amazon Picking Challenge</a>
</h2>

<p>I competed with Team Georgia Tech in the Amazon Picking challenge using a Schunk robot with two 7-DOF arms. We came in 10th place, internationally (40+ teams qualified, 28 attended). I designed and integrated the majority of the perception stack, including PCL in C++, Image analysis in MATLAB and various vision and logic components in Python. All of this was done using ROS and the various tools it provides, alongside numerous internally developed tools.</p>

<p>I competed as one of four members of Team Georgia Tech out of GT's Robotics and Intelligent Machines laboratory.</p>

<p>Our code is <a href="https://github.com/ehuang3/apc_ros">open source</a></p>

<h4>
<a id="implementation-details" class="anchor" href="#implementation-details" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation details</h4>

<p>Image-similarity metrics are generated using symmetrized Kullbeck-Leibler divergence (Jensen-Shannon) for initial object recognition in the 2D image. The object is segmented in 2D, and that segmentation is projected onto the point cloud - and is then used as a seed for region-growing 3D segmentation. Then, prerejective sample consensus is used to register our known models of the object to the sensed point cloud, which enables our robot to autonomously grasp the object.</p>

<h2>
<a id="visar" class="anchor" href="#visar" aria-hidden="true"><span class="octicon octicon-link"></span></a>VisAR</h2>

<p>VisAR (Visual Intelligence System via Augmented Reality, I didn't pick the name) uses an Oculus rift to display augmented reality to a user. </p>

<p>We use an FPGA to take in frames from two cameras mounted in front of the Rift, apply a barrel distortion to undo the Oculus Rift's lenses' pincushion distortion, and display that information to the user. At the same time, UI frames are being sent to the FPGA by a computer, and these are being overlayed with transparency onto the user's view.</p>

<p>I developed a Vispy/OpenGL based rendering engine called "libVisar" that is designed to make simple Oculus Rift applications very easy to make in Python. The idea is that roboticists or other engineers without an OpenGL/DirectX background, and without access to Unity can create Oculus Rift GUI's and data displays for overlay on camera images.  </p>

<p>Soon to be open source.</p>

<h2>
<a id="ieee2015-autonomous-robot-maverick-video-of-mavericks-arm" class="anchor" href="#ieee2015-autonomous-robot-maverick-video-of-mavericks-arm" aria-hidden="true"><span class="octicon octicon-link"></span></a>IEEE2015 Autonomous Robot "Maverick" <a href="http://youtu.be/6lzLW4y0CQY">Video of Maverick's Arm</a>
</h2>

<p>I led a 40 member team of undergraduate student engineers, building an autonomous robot for competition in the IEEE Southeastcon Competition. The goal is to build a fully autonomous robot that can navigate a randomized course to draw the word "IEEE" on an etch-a-sketch, rotate a rubix cube, successfully play <a href="http://dalpix.com/images/Simon-Game_l.jpg">"Simon Says"</a> and then pick up a card. We found this to be too easy, and decided to implement Chess as well.</p>

<p>We used monocular camera SLAM for navigation, a 4-DOF manipulator arm for acting on the environment (Using a second camera for visual servoing), mecanum wheels for locomotion, an Intel NUC running Ubuntu and ROS Indigo for high-level control, and an XMega128A1U for low-level control.</p>

<p>All of our code is completely <a href="github.com/ufieeehw/IEEE2015">open source</a>. If you are looking to this for inspiration and can't seem to make something work, please feel free to contact me.</p>

<p>For 2016, I have passed on leadership to Ian Van Stralen, with mechanical lead Stephan Strassle.</p>

<h2>
<a id="ieee2014-autonomous-robot-mr-roboto" class="anchor" href="#ieee2014-autonomous-robot-mr-roboto" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://youtu.be/b_Ijie9VGrQ">IEEE2014 Autonomous Robot "Mr. Roboto"</a>
</h2>

<p>We built a robot for the SoutheastCon competition in March of 2014. The robot had to autonomously drive to each of three blue blocks, wait three seconds, and then fire a dart into a target. We did not know the position of the blue blocks in advance. Most of the other teams relied on following the white lines (inc. Georgia Tech, Duke, etc). We, instead, scavenged a LIDAR from a Roomba (Well, a <a href="https://www.youtube.com/watch?v=KnspWPlBM_o">Neato XV-11</a>) and navigated using a particle filter. </p>

<p>I implemented a purely computer-vision method for identifying course objectives and determining the position of a target in the camera's field of view. At the beginning of the video, the robot rotates the turret head to visually identify the blue blocks it must drive to. Then, at each block, it servos to point toward the target based on its estimate of the position. Vision is then used to correct the target position estimate, and fire the dart accurately.</p>

<h2>
<a id="uf-eftp-bumper-bots" class="anchor" href="#uf-eftp-bumper-bots" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="http://imgur.com/a/TH279">UF EFTP Bumper Bots</a>
</h2>

<p>Brandon Grant and I built and programmed a set of remote-controlled bumper cars using RobotC and Lego NXT. "Bumper Bots" were designed for a UF Shands competition, to design a toy for ailing children at Shands hospital. We ended in the top three teams, and the kids adored our robots.</p>

<h1>
<a id="coursework" class="anchor" href="#coursework" aria-hidden="true"><span class="octicon octicon-link"></span></a>Coursework</h1>

<h2>
<a id="nonlinear-controls" class="anchor" href="#nonlinear-controls" aria-hidden="true"><span class="octicon octicon-link"></span></a>Nonlinear Controls</h2>

<p>I am currently taking Dr. Warren Dixon's graduate nonlinear controls course. So far, it has been largely a review of Tedrake's underactuated robotics.</p>

<h2>
<a id="medical-image-analysis" class="anchor" href="#medical-image-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Medical Image Analysis</h2>

<p>Graduate CS course at UF in medical image analysis. The course teaches methods behind CT and MRI reconstruction, and methods in medical image segmentation/registration.</p>

<h2>
<a id="underactuated-robotics" class="anchor" href="#underactuated-robotics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Underactuated Robotics</h2>

<p>MIT OCW's graduate Underactuated Robotics course. I've gained a lot from this course, and have been using tools gained in my daily work.</p>

<h2>
<a id="convex-optimization" class="anchor" href="#convex-optimization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Convex Optimization</h2>

<p>I am also currently working through (Albeit at a much lower priority) Dr. Stephen Boyd's Convex Optimization lectures from Stanford, at the encouragement of my friend Eric.</p>

<h1>
<a id="interests" class="anchor" href="#interests" aria-hidden="true"><span class="octicon octicon-link"></span></a>Interests</h1>

<p>Above all, I like jokes and movies about space. On the technical side, I am interested in autonomous systems, with preference to vehicles. Or really anything that you can convince me is a cool enough math/software problem. Key words: Perception, controls, autonomous, Linux, python, linear algebra, motion planning, model-predictive control, simulation, microcontrollers.</p>

      <footer class="site-footer">

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

            <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-63284725-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
